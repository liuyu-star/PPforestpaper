\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{lee2013pptree}
\citation{RCore}
\citation{breiman2001random}
\citation{breiman1996bagging,breiman1996heuristics}
\citation{amit1997shape,ho1998random}
\citation{lee2013pptree}
\citation{PPtreeViz}
\citation{friedman1973projection}
\citation{kim2001,brodley,tan2005mml,truong2009fast,lee2013pptree}
\citation{tan2006decision,menze2011oblique,do2010classifying}
\citation{Cortes1995}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\newlabel{sec:intro}{{1}{2}{Introduction}{section.1}{}}
\citation{friedman1973projection}
\citation{kruskal1969toward}
\citation{lee2005projection}
\citation{lee2010projection}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background on the projection pursuit tree}{3}{section.2}}
\newlabel{PPT}{{2}{3}{Background on the projection pursuit tree}{section.2}{}}
\citation{huber1990data}
\citation{PPtreeViz}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Behavior of the LDA index on 1D projections shown using a Huber plot for simulated data, two different sets of 2D. The solid line indicates index value for each 1D projection of the 2D data. The dashed circle indicates the median index value across all projections, and the dotted line corresponds to the projection producing the maximum index value. The histogram shows the projected data corresponding the maximum index value. \relax }}{4}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{huberpl}{{1}{4}{Behavior of the LDA index on 1D projections shown using a Huber plot for simulated data, two different sets of 2D. The solid line indicates index value for each 1D projection of the 2D data. The dashed circle indicates the median index value across all projections, and the dotted line corresponds to the projection producing the maximum index value. The histogram shows the projected data corresponding the maximum index value. \relax }{figure.caption.1}{}}
\citation{therneau2010rpart}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of decision boundaries for the \texttt  {rpart} (left) and PPtree (right) algorithms on 2D simulated data. The partitions generated by PPtree algorithm are oblique to the axis, incorporating the association between the two variables.\relax }}{5}{figure.caption.2}}
\newlabel{bounds}{{2}{5}{Comparison of decision boundaries for the \texttt {rpart} (left) and PPtree (right) algorithms on 2D simulated data. The partitions generated by PPtree algorithm are oblique to the axis, incorporating the association between the two variables.\relax }{figure.caption.2}{}}
\citation{breiman2001random}
\citation{Biau:2008}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of the PPtree algorithm for $g=3$ classes. It is a dual pass algorithm for multiclass problems, for each split. It first finds the best separation and combines classes into two super-groups. It then searches again for the best separation between these two super-groups and splits on this. It proceeds sequentially on the subsets in the nodes, but only $g-1$ splits are allowed. \relax }}{7}{figure.caption.3}}
\newlabel{diagpp1}{{3}{7}{Illustration of the PPtree algorithm for $g=3$ classes. It is a dual pass algorithm for multiclass problems, for each split. It first finds the best separation and combines classes into two super-groups. It then searches again for the best separation between these two super-groups and splits on this. It proceeds sequentially on the subsets in the nodes, but only $g-1$ splits are allowed. \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Projection pursuit random forest}{7}{section.3}}
\newlabel{PPFsec}{{3}{7}{Projection pursuit random forest}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Definition}{7}{subsection.3.1}}
\citation{breiman2001random}
\citation{athreya2006measure}
\newlabel{rfesti}{{3}{8}{Definition}{equation.3.3}{}}
\newlabel{predfor}{{4}{8}{Definition}{equation.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Algorithm }{8}{subsection.3.2}}
\citation{PPtreeViz}
\citation{PPforestpkg}
\citation{profvis}
\citation{eddelbuettel2011rcpp}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Illustration of the PPforest algorithm. It is effectively the same as a random forest algorithm except that the PPtree classifier is used on each bootstrap sample. \relax }}{10}{figure.caption.4}}
\newlabel{diagppf}{{4}{10}{Illustration of the PPforest algorithm. It is effectively the same as a random forest algorithm except that the PPtree classifier is used on each bootstrap sample. \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Implementation}{10}{subsection.3.3}}
\newlabel{impl}{{3.3}{10}{Implementation}{subsection.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Computational performance for different sample size ($n$), number of variables ($p$), number of groups ($g$), number of trees ($m$) and proportion of variables considered for each node. Computational speed suffers most when the proportion of variables is high.\relax }}{11}{figure.caption.5}}
\newlabel{ratiotim}{{5}{11}{Computational performance for different sample size ($n$), number of variables ($p$), number of groups ($g$), number of trees ($m$) and proportion of variables considered for each node. Computational speed suffers most when the proportion of variables is high.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}PPF diagnostics}{11}{subsection.3.4}}
\citation{breiman2001random}
\citation{lee2013pptree}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Model error}{12}{subsubsection.3.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Variable importance}{12}{subsubsection.3.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Observational level diagnostics}{13}{subsubsection.3.4.3}}
\citation{Lichman}
\@writefile{toc}{\contentsline {section}{\numberline {4}Performance comparison}{14}{section.4}}
\newlabel{perfsec}{{4}{14}{Performance comparison}{section.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Overview of benchmark data: number of cases, predictors, groups, imbalance and correlation. Imbalance indicates relative class sizes (0=balanced classes), and higher correlation indicates the potential for separations occurring in combinations of variables.\relax }}{14}{table.caption.6}}
\newlabel{bench.tab}{{1}{14}{Overview of benchmark data: number of cases, predictors, groups, imbalance and correlation. Imbalance indicates relative class sizes (0=balanced classes), and higher correlation indicates the potential for separations occurring in combinations of variables.\relax }{table.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of CART, PPtree, RF and PPF results with various data sets. The mean of training and test error rates from 200 re-samples is shown. (Order of rows is same as in Table \ref  {bench.tab}.) PPF performs favorably compared to the other methods. \relax }}{15}{table.caption.7}}
\newlabel{res}{{2}{15}{Comparison of CART, PPtree, RF and PPF results with various data sets. The mean of training and test error rates from 200 re-samples is shown. (Order of rows is same as in Table \ref {bench.tab}.) PPF performs favorably compared to the other methods. \relax }{table.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Benchmark data results shown graphically. PPF performs consistently well across most of the data sets. \relax }}{16}{figure.caption.8}}
\newlabel{parallel}{{6}{16}{Benchmark data results shown graphically. PPF performs consistently well across most of the data sets. \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Diagnostics comparison}{16}{section.5}}
\newlabel{options}{{5}{16}{Diagnostics comparison}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Variable importance}{16}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Comparison of importance measures for the lymphoma data, where PPF outperformed RF. Top row shows the top 10 variables by each method, with two ways of calculating with PPF. Bottom row shows the top two variables from each, which illustrates the difference between methods. PPF is detecting differences between groups when there is association between variables. Using the global average importance (middle), Gene35 and Gene50 better distinguish group FL. Using the global importance, Gene35 and Gene44 find a big gap between group DLBCL and the other two. \relax }}{17}{figure.caption.9}}
\newlabel{globalimp}{{7}{17}{Comparison of importance measures for the lymphoma data, where PPF outperformed RF. Top row shows the top 10 variables by each method, with two ways of calculating with PPF. Bottom row shows the top two variables from each, which illustrates the difference between methods. PPF is detecting differences between groups when there is association between variables. Using the global average importance (middle), Gene35 and Gene50 better distinguish group FL. Using the global importance, Gene35 and Gene44 find a big gap between group DLBCL and the other two. \relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Vote matrix}{17}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison of the out-of-bag vote matrix for the three groups of the lymphoma data, returned by RF and PPF: (top) ternary plot, (bottom) side-by-side jittered dotplots. This illustrates the difference between methods. PPF votes more decisively for most cases, than RF. Especially this is true for the DLCBL class, where all but one are almost always predicted to the true class.\relax }}{18}{figure.caption.10}}
\newlabel{voteplots}{{8}{18}{Comparison of the out-of-bag vote matrix for the three groups of the lymphoma data, returned by RF and PPF: (top) ternary plot, (bottom) side-by-side jittered dotplots. This illustrates the difference between methods. PPF votes more decisively for most cases, than RF. Especially this is true for the DLCBL class, where all but one are almost always predicted to the true class.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Proximity}{19}{subsection.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Parameter selection}{19}{subsection.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Examining similarity between cases, using pairwise plots of multidimensional scaling on the proximity matrix from RF and PPF fits of the lymphoma data. It can be seen that most cases are grouped closely with their class in PPF while in RF FL and B-CLL are mixed. \relax }}{20}{figure.caption.11}}
\newlabel{prox1}{{9}{20}{Examining similarity between cases, using pairwise plots of multidimensional scaling on the proximity matrix from RF and PPF fits of the lymphoma data. It can be seen that most cases are grouped closely with their class in PPF while in RF FL and B-CLL are mixed. \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Application: RNA-seq gene expression}{20}{section.6}}
\newlabel{rnaapli}{{6}{20}{Application: RNA-seq gene expression}{section.6}{}}
\citation{paschold2012}
\citation{datta2014statistical}
\citation{cook2007exploring}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Illustrating model tuning using error rate reduction. The average error rate plotted against proportion of variables in all the benchmark data is shown at left. The error rate tends to be better with more variables, but it does vary substantially by data set. OOB error is plotted against number of trees (right) on the lymphoma data for both PPF and RF. PPF has the consistently lower error, but both would indicate about 100 trees is sufficient to get the best results.  \relax }}{21}{figure.caption.13}}
\newlabel{parameters}{{10}{21}{Illustrating model tuning using error rate reduction. The average error rate plotted against proportion of variables in all the benchmark data is shown at left. The error rate tends to be better with more variables, but it does vary substantially by data set. OOB error is plotted against number of trees (right) on the lymphoma data for both PPF and RF. PPF has the consistently lower error, but both would indicate about 100 trees is sufficient to get the best results.  \relax }{figure.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of CART, PPF, PPtree, and RF results with maize RNA-seq gene exression data set. The mean of training and test error rates from leave-one-out cross validation is shown. PPF.03 performs favorably compared to the other methods. \relax }}{22}{table.caption.14}}
\newlabel{res_RNA}{{3}{22}{Comparison of CART, PPF, PPtree, and RF results with maize RNA-seq gene exression data set. The mean of training and test error rates from leave-one-out cross validation is shown. PPF.03 performs favorably compared to the other methods. \relax }{table.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Overview of the PPF results for the maize RNA-seq data. The parallel coordinate plot (top left) shows the expression values for 10 most important genes. A dotplot shows the global average importance for the top 200 genes (top right). The three scatterplots show pairs of important variables (bottom row), indicating clear separation between classes. \relax }}{22}{figure.caption.15}}
\newlabel{rnacomp}{{11}{22}{Overview of the PPF results for the maize RNA-seq data. The parallel coordinate plot (top left) shows the expression values for 10 most important genes. A dotplot shows the global average importance for the top 200 genes (top right). The three scatterplots show pairs of important variables (bottom row), indicating clear separation between classes. \relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{23}{section.7}}
\newlabel{discpp1}{{7}{23}{Discussion}{section.7}{}}
\citation{xie:2015}
\citation{hadley:2009}
\citation{dplyr}
\bibstyle{jasa3}
\bibdata{ppforestpaperbib}
\bibcite{amit1997shape}{{1}{1997}{{Amit and Geman}}{{Amit and Geman}}}
\bibcite{athreya2006measure}{{2}{2006}{{Athreya and Lahiri}}{{Athreya and Lahiri}}}
\bibcite{Biau:2008}{{3}{2008}{{Biau et~al.}}{{Biau, Devroye, and Lugosi}}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Acknowledgements}{24}{section.8}}
\newlabel{Aknow}{{8}{24}{Acknowledgements}{section.8}{}}
\bibcite{breiman1996bagging}{{4}{1996{a}}{{Breiman}}{{Breiman}}}
\bibcite{breiman1996heuristics}{{5}{1996{b}}{{Breiman}}{{Breiman}}}
\bibcite{breiman2001random}{{6}{2001}{{Breiman}}{{Breiman}}}
\bibcite{brodley}{{7}{1995}{{Brodley and Utgoff}}{{Brodley and Utgoff}}}
\bibcite{profvis}{{8}{2016}{{Chang and Luraschi}}{{Chang and Luraschi}}}
\bibcite{cook2007exploring}{{9}{2007}{{Cook et~al.}}{{Cook, Hofmann, Lee, Yang, Nikolau, and Wurtele}}}
\bibcite{Cortes1995}{{10}{1995}{{Cortes and Vapnik}}{{Cortes and Vapnik}}}
\bibcite{PPforestpkg}{{11}{2018}{{da~Silva et~al.}}{{da~Silva, Lee, and Cook}}}
\bibcite{datta2014statistical}{{12}{2014}{{Datta and Nettleton}}{{Datta and Nettleton}}}
\bibcite{do2010classifying}{{13}{2010}{{Do et~al.}}{{Do, Lenca, Lallich, and Pham}}}
\bibcite{eddelbuettel2011rcpp}{{14}{2011}{{Eddelbuettel et~al.}}{{Eddelbuettel, Fran{\c {c}}ois, Allaire, Chambers, Bates, and Ushey}}}
\bibcite{friedman1973projection}{{15}{1974}{{Friedman and Tukey}}{{Friedman and Tukey}}}
\bibcite{ho1998random}{{16}{1998}{{Ho}}{{Ho}}}
\bibcite{huber1990data}{{17}{1990}{{Huber}}{{Huber}}}
\bibcite{kim2001}{{18}{2001}{{Kim and Loh}}{{Kim and Loh}}}
\bibcite{kruskal1969toward}{{19}{1969}{{Kruskal}}{{Kruskal}}}
\bibcite{PPtreeViz}{{20}{2018}{{Lee}}{{Lee}}}
\bibcite{lee2010projection}{{21}{2010}{{Lee and Cook}}{{Lee and Cook}}}
\bibcite{lee2005projection}{{22}{2005}{{Lee et~al.}}{{Lee, Cook, Klinke, and Lumley}}}
\bibcite{lee2013pptree}{{23}{2013}{{Lee et~al.}}{{Lee, Cook, Park, and Lee}}}
\bibcite{Lichman}{{24}{2013}{{Lichman}}{{Lichman}}}
\bibcite{menze2011oblique}{{25}{2011}{{Menze et~al.}}{{Menze, Kelm, Splitthoff, Koethe, and Hamprecht}}}
\bibcite{paschold2012}{{26}{2012}{{Paschold et~al.}}{{Paschold, Jia, Marcon, Lund, Larson, Yeh, Ossowski, Lanz, Nettleton, Schnable et~al.}}}
\bibcite{RCore}{{27}{2019}{{{R Core Team}}}{{{R Core Team}}}}
\bibcite{tan2005mml}{{28}{2004}{{Tan and Dowe}}{{Tan and Dowe}}}
\bibcite{tan2006decision}{{29}{2006}{{Tan and Dowe}}{{Tan and Dowe}}}
\bibcite{therneau2010rpart}{{30}{2010}{{Therneau et~al.}}{{Therneau, Atkinson, and Ripley}}}
\bibcite{truong2009fast}{{31}{2009}{{Truong}}{{Truong}}}
\bibcite{hadley:2009}{{32}{July 2009}{{Wickham}}{{Wickham}}}
\bibcite{dplyr}{{33}{2015}{{Wickham et~al.}}{{Wickham, Francois, and Rstudio}}}
\bibcite{xie:2015}{{34}{2015}{{Xie}}{{Xie}}}
